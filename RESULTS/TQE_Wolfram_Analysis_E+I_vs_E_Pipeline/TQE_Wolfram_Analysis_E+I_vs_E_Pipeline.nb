(* SPDX-License-Identifier: MIT *)
(* Copyright (c) 2025 Stefan Len *)

(* =================================================================================== *)
(* TQE_Wolfram_Analysis_E+I_vs_E_Pipeline.nb *)
(* =================================================================================== *)
(* Author: Stefan Len *)
(* =================================================================================== *)

(* ============================= *)
(* 1) SET INPUT DIRECTORIES      *)
(* ============================= *)

dirA = "/Users/stevilen/Desktop/TQE_COMBINED_CSV/dataset_A_E+I";
dirB = "/Users/stevilen/Desktop/TQE_COMBINED_CSV/dataset_B_E_only";

(* ============================= *)
(* 2) HELPER FUNCTIONS           *)
(* ============================= *)

ClearAll[say, safeImport, toDS, pickCol, binStats, pct, niceName, tryCols, colExistsQ];

(* Pretty printing for headers *)
say[txt_] := Print[Style[txt, 14, Bold, RGBColor[0.1, 0.2, 0.6]]];

(* read CSV as header-aware Dataset (associations per row) *)
safeImport[f_] := Quiet @ Check[
  Import[f, "Dataset"],
  Dataset[{}]
];

(* Ensure input is a Dataset *)
toDS[x_] := Quiet @ Check[
  Which[
    Head[x] === Dataset, x,
    ListQ[x] && Length[x] > 0, Dataset[x],
    True, Dataset[{}]
  ],
  Dataset[{}]
];

(* Check if a column exists in dataset *)
colExistsQ[ds_, name_] := Module[{ok = MatchQ[ds, _Dataset] && Length[ds] > 0},
  If[!ok, False, Quiet@Check[KeyExistsQ[Normal@First@ds, name], False]]
];

(* Try multiple column name candidates *)
tryCols[ds_, names_List] := With[
  {hit = SelectFirst[names, colExistsQ[ds, #] &, None]},
  If[hit === None, Missing["NotAvailable"], ds[All, hit]]
];

(* Format percentage nicely *)
pct[num_, den_] := If[den > 0, NumberForm[100.0*num/den, {4, 2}], Missing["NA"]];

(* Normalize file base name for pairing *)
niceName[path_, tag_] := Module[{b = FileBaseName[path]},
  StringReplace[b, {
    "_E+I" -> "", "_E-Only" -> "", "_E_only" -> "", "_EOnly" -> "",
    "_E" -> "", tag -> ""
  }]
];

(* pick the main summary table *)
pickSummaryFile[asc_] := Module[{keys = Keys@asc, hits},
  hits = Select[keys, StringContainsQ[#, 
           {"stability", "metrics__stability", "stability__cls", "metrics_stability"},
           IgnoreCase -> True] &];
  If[hits =!= {}, 
     First@MaximalBy[hits, Length@toDS@asc[#] &],
     If[keys === {}, Missing["NoFile"],
       First@MaximalBy[keys, Module[{ds = toDS@asc[#]},
         Which[
           !MatchQ[ds, _Dataset] || Length@ds == 0, -Infinity,
           True, Log[1+Length@ds]
         ]
       ] &]
     ]
  ]
];

(* Column name variants *)
stableCol = tryCols[#, {"stable","is_stable","stable_flag","stable_cls","stability"}] &;
lockCol   = tryCols[#, {"lock_epoch","lockin_epoch","lock_ep","lock_step","lockin_step"}] &;
xCol      = tryCols[#, {"X","x","E_times_I","E_mul_I"}] &;
eCol      = tryCols[#, {"E","e"}] &;
tCol      = tryCols[#, {"time_step","t","step","epoch"}] &;
geCol     = tryCols[#, {"global_entropy","entropy_global","H_global","entropy"}] &;

(* Calculate the binned stability curve *)
binStats[ds_, var_, nbins_:40] := Module[
  {varname = ToString[var], v, st, data, min, max, edges, idx, grp, means},
  v  = tryCols[ds, {varname}];
  st = tryCols[ds, {"stable","is_stable","stable_flag","stable_cls","stability"}];
  If[MissingQ[v] || MissingQ[st], Return[Missing["NoData"]]];
  data = Cases[Transpose[{Normal@v, Normal@st}], {x_?NumericQ, y_?NumericQ}];
  If[data === {}, Return[Missing["NoData"]]];
  {min, max} = {Min[data[[All,1]]], Max[data[[All,1]]]};
  If[!NumericQ[min] || !NumericQ[max] || min==max, Return[Missing["NoData"]]];
  edges = Subdivide[min, max, nbins];
  idx   = Clip[Floor@Rescale[data[[All,1]], {min, max}, {1, nbins}], {1, nbins}];
  grp   = GroupBy[Transpose[{idx, data[[All,2]]}], First -> Last];
  means = Table[If[KeyExistsQ[grp,i], Mean@grp[i], Missing["NA"]], {i,1,nbins}];
  Dataset @ Table[
    <|"bin"->i, "edge_min"->edges[[i]], "edge_max"->edges[[i+1]], "stability_rate"->means[[i]]|>,
    {i,1,nbins}
  ]
];

(* collect files and combine datasets *)
findFiles[asc_, patterns_List] :=
  Select[Keys@asc, StringContainsQ[#, patterns, IgnoreCase -> True] &];

combineDS[asc_, files_List] := Module[
  {rows = DeleteMissing[toDS /@ (asc /@ files)]},
  If[rows === {}, Dataset[{}], Dataset @ Flatten[Normal /@ rows, 1]]
];

(* std dev of global_entropy after lock epoch *)
postLockStd[ds_] := Module[
  {lock = tryCols[ds, {"lock_epoch","lockin_epoch","lock_ep","lock_step","lockin_step"}],
   ge   = tryCols[ds, {"global_entropy","entropy_global","H_global","entropy"}],
   t    = tryCols[ds, {"time_step","t","step","epoch"}], lvList, lv, tt, gg},
  If[ Or[MissingQ[lock], MissingQ[ge], MissingQ[t]], Return[Missing["NA"]] ];
  lvList = DeleteMissing@Normal@lock;
  If[lvList === {}, Return[Missing["NA"]]];
  lv = First@lvList; tt = Normal@t; gg = Normal@ge;
  If[ !NumericQ[lv], Return[Missing["NA"]] ];
  With[{mask = Boole[tt >= lv]},
    If[ Total@mask < 5, Missing["NA"], StandardDeviation@Pick[gg, mask, 1] ]
  ]
];

(* ============================= *)
(* 3) LOAD AND PAIR FILES        *)
(* ============================= *)

filesA = FileNames["*.csv", dirA, Infinity];
filesB = FileNames["*.csv", dirB, Infinity];

assocA = AssociationMap[safeImport, filesA];
assocB = AssociationMap[safeImport, filesB];

keyA = AssociationThread[niceName[#, "A"] & /@ Keys[assocA] -> Keys[assocA]];
keyB = AssociationThread[niceName[#, "B"] & /@ Keys[assocB] -> Keys[assocB]];

commonKeys = Intersection[Keys[keyA], Keys[keyB]];

say["File Loading Summary"];
Grid[{
  {"A (E+I) files found", Length[assocA]},
  {"B (E-only) files found", Length[assocB]},
  {"Matched file pairs", Length[commonKeys]}
}, Frame -> All]

(* ============================= *)
(* 6) CMB ANOMALIES (COLD SPOT)   *)
(* ============================= *)

say["3) CMB cold-spots"];

coldAFiles = findFiles[assocA, {"cmb_coldspots","coldspots_summary","coldspot","cold_spot"}];
coldBFiles = findFiles[assocB, {"cmb_coldspots","coldspots_summary","coldspot","cold_spot"}];

coldA = combineDS[assocA, coldAFiles];
coldB = combineDS[assocB, coldBFiles];

coldStats[ds_] := Module[{z = tryCols[ds, {"z_value", "z", "depth_uK", "depth_microK"}], v},
  If[MissingQ[z], <|"count"->0, "min"->Missing["NA"], "mean"->Missing["NA"], "median"->Missing["NA"], "below_-70"->Missing["NA"]|>,
    v = DeleteMissing@Normal@z;
    If[v === {}, <|"count"->0, "min"->Missing["NA"], "mean"->Missing["NA"], "median"->Missing["NA"], "below_-70"->Missing["NA"]|>,
      <|"count" -> Length[v], 
        "min" -> N[Min[v]], "mean" -> N[Mean[v]], "median" -> N[Median[v]], 
        "below_-70" -> Count[v, x_ /; x <= -70]|>
    ]
  ]
];

cA = If[MatchQ[coldA, _Dataset], coldStats[coldA], <||>];
cB = If[MatchQ[coldB, _Dataset], coldStats[coldB], <||>];

Grid[{
    {"Metric", "E+I (A)", "E-only (B)"},
    {"Total Spots Found", cA["count"], cB["count"]},
    {"Deepest Spot (µK or z)", cA["min"], cB["min"]},
    {"Average Depth", cA["mean"], cB["mean"]},
    {"Median Depth", cA["median"], cB["median"]},
    {"Count ≤ -70 µK", cA["below_-70"], cB["below_-70"]}
}, Frame -> All]

(* ============================= *)
(* 11) SAVE RESULTS TO DISK      *)
(* ============================= *)

CleanForJSON[x_] := x /. Missing[_] -> Null;

say["11) Saving results to disk"];

nbDir   = Quiet@NotebookDirectory[];
baseDir = If[StringQ[nbDir], nbDir, FileNameJoin[{$HomeDirectory, "Desktop"}]];
outDir  = FileNameJoin[{baseDir, "TQE_Wolfram_Analysis_Output"}];
If[!DirectoryQ[outDir],
  CreateDirectory[outDir, CreateIntermediateDirectories -> True]
];

Export[FileNameJoin[{outDir, "coldspot_stats.json"}],
  CleanForJSON @ Normal @ <|"E+I" -> cA, "E-only" -> cB|>
];

say["Results exported to: " <> outDir];
